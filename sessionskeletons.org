* calibration
** description
   really cool
   brainflow returns "uV" (micro Volts probably?)
   
** subject roles
*** viewer
**** stimuli
     a deap dataset video
**** sensors
     ultracortex
**** other notes
     the movie is so cool
**** survey
     fuck you
** nwb unit columns
*** valence
    the level of valence self reported by the subject after watching the video, on a scale from 1-9. sam rating scale image presented with the question -- experiment-media/sam.png
*** exitation
    the level of valence self reported by the subject after watching the video, on a scale from 1-9. sam rating scale image presented with the question -- experiment-media/sam.png
*** videourl
    URL to the DEAP video
** programmatic
*** initial
#+BEGIN_SRC python
  # programmtic implementation based on
  #https://content.iospress.com/download/technology-and-health-care/thc174836?id=technology-and-health-care%2Fthc174836
  
  # ACTUAL TODO add survey about DEAPU
  # ACTUAL TODO add / look into adding nwb processing modules
  # ACTUAL TODO look into making org file in subject folder detailing their progress with the DEAP videos
  from brainflow.data_filter import DataFilter, FilterTypes, AggOperations
  from scipy.stats import entropy
  import pandas as pd
  print('Please select a DEAP video for the subject to watch now')
  video_link = input('Please enter the url for the DEAP video...')
  print('Please prepare subject for viewing session:')
  print('  - in a separate terminal window navigate to `experiment-media` in your nwborg project root folder and run `feh SAM.png`')
  print('  - Look at the timestamps for the video specified in the DEAP dataset, prepare to play the video starting at the appropriate timestamp')
  print('  - Using a timer or watching the video progress bar, prepare to stop the video at the appropriate timestamp\n')
  input("When you're ready: press Enter in this window. The recording session will begin. Wait 3 seconds and then press the play button to begin playing the video")
  subject_id = session_dict['subject roles']['viewer']['subject id']
  
  if os.path.isfile('subjects/' + str(subject_id) + '/calibration_knn.org'):
      session_knn_points = orgutils.orgToDict(filename=('subjects/' + str(subject_id) + '/calibration_knn.org'))
  else:
      session_knn_points = {}
  feature_point_list = []
  eeg_data_channel_list = []
#+END_SRC
*** loop
#+BEGIN_SRC python
  eeg_data_channel_list = [channel_1,channel_2,channel_3,channel_4,channel_5,channel_6,channel_7,channel_8]
#+END_SRC       
*** terminal
#+BEGIN_SRC python
  # actual todo add arguments for the different dimensions
  # session_path
  #if os.path.isfile('subjects/' + str(subject_id) + '/calibration_knn.csv'):
  #    calibration_csv = pd.read_csv('subjects/' + str(subject_id) + '/calibration_knn.csv') # read in the csv 
  #else:
  #    calibration_csv = pd.Dataframe(columns=['alpha_entropy','alpha_energy','beta_entropy','beta_energy','gamma_entropy','gamma_energy','theta_entropy','theta_energy','valence','exitement'])
  # calibration_csv.to_csv('subjects/' + str(subject_id) + '/calibration_knn.csv')
  # enter all the calibration data into the dataframe as rows
  
  
  print('\n(questions for the subject)')
  print('please direct your attention to the emotional scale image and answer the following questions based on your experience watching the video:')
  session_knn_points[video_link] = {}
  sam_valence = input('where do you fall on the top row scale? left to right 1-9, top row (valence)...')
  session_knn_points[video_link]['VALENCE'] = sam_valence
  sam_excitation = input('where do you fall on the middle row scale? left to right 1-9 middle row (excitation)...')
  session_knn_points[video_link]['EXCITATION'] = sam_excitation
  nwbfile.add_unit(id=1,valence=int(sam_valence),exitation=int(sam_excitation),videourl=video_link)
  
  
  
  # Pick it up, what needs to happen is nwb_eeg_ts needs to be iterated over with half windows of 500 (2 seconds)
  # With features being captured at resolutions of 1 window or 4 seconds 1000 points
  # basically the code from loop translated to be at the end when looping over all this shtuff
  
  half_window_count = int(len(nwb_eeg_ts_raw)/500) # the number of half windows across the frame of the session
  #print(len(nwb_eeg_ts.data))
  #print(len(nwb_eeg_ts_raw))
  
  # use channel list
  for window_idx in range(half_window_count):
      if bool(window_idx): # if it isn't the 0 index window
          knn_feature_point = {}
          for channel_number, channel in enumerate(eeg_data_channel_list):
              numpy_channel = np.array(channel)
  
              # ACTUAL TODO TEST vvvvv change back or investigate further
              normalized_channel = (numpy_channel - numpy_channel.min()) / (numpy_channel.max() - numpy_channel.min())
              #^^^^^^ using min-max normalization ^^^^^^
              window_data = normalized_channel[(window_idx * 500):((window_idx * 500) + 1000)]
              alpha_band = window_data.copy()
              beta_band = window_data.copy()
              gamma_band = window_data.copy()
              theta_band = window_data.copy()
              #print('before theta bandpass:\n',window_data)
              DataFilter.perform_bandpass(data=theta_band,sampling_rate=250,center_freq=6.0,band_width=4.0,order=1,filter_type=0,ripple=0.0)
              #print('after theta bandpass:\n',window_data,'\n\n\n')
              DataFilter.perform_bandpass(data=alpha_band,sampling_rate=250,center_freq=12.0,band_width=8.0,order=1,filter_type=0,ripple=0.0)
              DataFilter.perform_bandpass(data=beta_band,sampling_rate=250,center_freq=24.0,band_width=16.0,order=1,filter_type=0,ripple=0.0)
              DataFilter.perform_bandpass(data=gamma_band,sampling_rate=250,center_freq=48.0,band_width=32.0,order=1,filter_type=0,ripple=0.0)
  
              print(alpha_band)
              print(beta_band)
              print(gamma_band)
              print(theta_band)
              # if needed you should convert the bands to np again if datafilter doesn't return an np array
              # REAL ACTUAL TODO you gotta do DWT
              #print('alpha band: ', alpha_band)
              #print(entr(list(alpha_band)))
              knn_feature_point['channel' + str(channel_number)] = {'alpha' : {'entropy' : entropy(np.square(alpha_band)),
                                                                               'energy'  : alpha_band.sum() * .004},
                                                                    'beta'  : {'entropy' : entropy(np.square(beta_band)),
                                                                               'energy'  : beta_band.sum() * .004},
                                                                    'gamma' : {'entropy' : entropy(np.square(gamma_band)),
                                                                               'energy'  : gamma_band.sum() * .004},
                                                                    'theta' : {'entropy' : entropy(np.square(theta_band)),
                                                                               'energy'  : theta_band.sum() * .004}}
              #beta_features = [beta_band/beta_band.sum(axis=1,keepdims=True),]
              #gamma_features = [gamma_band/gamma_band.sum(axis=1,keepdims=True),]
              #theta_features = [theta_band/theta_band.sum(axis=1,keepdims=True),]
              session_knn_points[video_link][str(window_idx)] = knn_feature_point
              orgutils.dictToOrg(org_data=session_knn_points,output_filename=('subjects/' + str(subject_id) + '/calibration_knn.org'))
  
#+END_SRC       
* emotive controller
** description
   mario sunshine get pumped
** subject roles
*** player
**** stimuli
     super mario sunshine
**** sensors
     ultracortex
**** other notes
     level 2
**** survey
     fuck you
** programmatic
*** initial
#+begin_src python
  from brainflow.data_filter import datafilter
  # emotive controller initial:
  parser.add_argument('--pipe-path', type=str, help='the path to the controller input pipe',
                        required=false, default='/home/shaneallcroft/.local/share/dolphin-emu/pipes/pipe1')
#+end_src
*** loop
#+begin_src python
  # emotive controller loop:
  # requires sensors/ultracortex.org
  coefficients = datafilter.perform_wavelet_transform(egg_data)
  
#+end_src
*** terminal
#+begin_src python
  # emotive controller terminal:
#+end_src
